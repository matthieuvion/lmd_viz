{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Le Monde, 1 y. of comments - Ukraine War'\n",
    "subtitle: As a (partial) proxy to measure people engagement\n",
    "author:\n",
    "  name: github.com/vionmatthieu/lmd_viz\n",
    "  url: 'https://github.com/vionmatthieu/lmd_viz'\n",
    "execute:\n",
    "  freeze: auto\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-location: left\n",
    "    topc-depth: 2\n",
    "    theme: spacelab\n",
    "    grid:\n",
    "      margin-width: 320px\n",
    "    embed-resources: true\n",
    "    code-overflow: wrap\n",
    "reference-location: margin\n",
    "citation-location: margin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside\n",
    "\n",
    "As a reader of *Le Monde* ---and the comments section ;) I would regularly encounter familiar subscribers' names. One in particular --more on that later, would *manually* keep track, count & cite \"pro-russian\" contributors, directly in the comments. That triggered my need to collect and perform some analysis in a bit more data-science oriented fashion.\\\n",
    "\\\n",
    "After our initial data collection[^1] we use `Polars` as an alternative to `Pandas` (still used in some parts when we had to code faster) to perform aggregations and `Plotly` to visualize[^2].\n",
    "\n",
    "[^1]: Custom API, dataset & scope on my other [project](https://github.com/matthieuvion/lmd_ukr)\n",
    "\n",
    "[^2]: This article itself = ipynb ([source notebook](https://github.com/matthieuvion/lmd_viz)) -\\> qmd -\\> html , via Quarto\n",
    "\n",
    "The analysis focuses on comments/authors (big numbers, activity over time, cohort analysis...) rather than on articles & titles. To this end, we also lay the foundations to go deeper in the semantic analysis through semantic search on comments via `Sbert` embedding + a `Faiss` index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Polars / Plotly config. to better render our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| output: false\n",
    "# Polars, render text columns nicer when printing / displaying df\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "pl.Config.set_tbl_cols(10)\n",
    "pl.Config.set_tbl_width_chars(120)\n",
    "pl.Config.set_tbl_rows(10)\n",
    "pl.Config.set_tbl_hide_dataframe_shape(True) # prevents systematic display of df/table shape\n",
    "\n",
    "# change default plotly express theme\n",
    "import plotly.io as pio\n",
    "\n",
    "print(f\" reminder plotly quick templates : {pio.templates}\")\n",
    "template = \"simple_white\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "236k comments collected, with associated articles & titles \\| 24th feb 2022 - 24 feb 2023\\\n",
    "Reminder : conflict starts Febr the 24th 2022, if we exclude the prior Dombass \"events\".\\\n",
    "Load our .parquet dataset[^3] using Polars.\n",
    "\n",
    "[^3]: Used keywords, [scope](https://github.com/matthieuvion/lmd_ukr) and limitations of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet using Polars. Could also use scan + collect syntax for lazy execution\n",
    "# If interested, I did some speed benchmark in the dataset project (lmd_ukr).\n",
    "filepath = \"data/lmd_ukraine.parquet\"\n",
    "coms = pl.read_parquet(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page\n",
    "#| echo: false\n",
    "\n",
    "# Can use either print(df) or display(df) or df.head(). In Polars, print displays a nice tabular df.\n",
    "# Polars displays column types by default <3\n",
    "# print(f\"dtypes: {coms.dtypes}\"). Not needed unless you're having a lot of cols.\n",
    "print(f\"shape: {coms.shape}\")\n",
    "print(coms.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of magnitude\n",
    "\n",
    "One always likes to know the \"how big\". For instance, Le Monde regularly get asked how many questions people would send during live sessions etc. We do not have those numbers, but articles / coms counts are still nice to have. Curious of which metrics (for comments) are available to *Le Monde* behind the scene. Probably a lot.\n",
    "\n",
    "#### Unique articles, comments count, unique authors\n",
    "\n",
    "236k comments from 10 500 unique subscribers, under 2600 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars methods are quite similar to Pandas except for the slicing.\n",
    "\n",
    "# number of comments; could also simply use .shape\n",
    "count = coms.select([pl.col(\"comment\").count()]).to_series()[0]\n",
    "\n",
    "# number of unique articles\n",
    "nunique_articles = coms.select(\"article_id\").n_unique()\n",
    "\n",
    "# n unique comments' authors\n",
    "nunique_authors = coms.select(\"author\").n_unique()\n",
    "\n",
    "print(f\"Number of comments: {count},\\nUnique articles: {nunique_articles},\\nUnique authors: {nunique_authors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note appearance=\"simple\"}\n",
    "After some googling : *Le Monde* has +-450k online subscribers (+-540k total). The comment section is open to subscribers only.\\\n",
    "10 500 unique authors means that around **2,3% of the reader base** have engaged in the comment section during the year, on this topic. Not surprising but not bad either, purely by rule of thumb -- a bench. would be interesting.\n",
    ":::\n",
    "\n",
    "#### Editorial & comment activity, elements of comparison\n",
    "\n",
    "Our dataset excludes the \"Lives\" that represent a substancial coverage effort from *Le Monde*. But just by reading the newspaper (or any, really) we know that they have been mobilizing a lot of resources. Also, regularly lurking into comments section, I know the topic is rather engaging. Now we have the accurate numbers, at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# Activity avg posts or comments per day\n",
    "days = 365\n",
    "total_articles = 2590\n",
    "total_comments = 236643\n",
    "\n",
    "# articles, (excluding lives/blogs posts) per day\n",
    "print(f\"Theme, Ukraine conflict:\")\n",
    "print(f\" - avg articles per day: {total_articles/days:.2f}\")\n",
    "\n",
    "# comments per day\n",
    "print(f\" - avg comments per day: {total_comments/days:.2f}\")\n",
    "\n",
    "# avg n comments per article\n",
    "print(f\" - avg comments per article: {total_comments/total_articles:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine publishing 7 articles a day on a topic, *for one year*. To put some perspective on our data, I performed a side & quick additional scraping on two additional topics. Articles count is exhaustive (on a given 1 month period) whereas comments activity was sampled from a random selection of articles for each topic to advance quickly.\n",
    "\n",
    "-   \"Réforme retraites\" : very hot topic during with \"hottest\" (demonstrations, strikes) month coverage happening in the same time span as the conflict.\n",
    "-   \"Trump\" : an always hot/engaging topic in any media, though a bit out of fashion nowadays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# Collected benchmark data\n",
    "themes = [\"réforme retraites\", \"Donald trump\"]\n",
    "n_articles = [\n",
    "    374,\n",
    "    66,\n",
    "]  # obtained on 1 month data (jan/febr 2023, exhaustive/no sampling)\n",
    "n_days = 31\n",
    "from_sample_avg_comments_per_articles = [\n",
    "    124,\n",
    "    40,\n",
    "]  # obtained from a sample of 20 articles for each theme.\n",
    "\n",
    "for idx, theme in enumerate(themes):\n",
    "    print(f\"Theme, {theme}:\")\n",
    "    print(f\" - avg articles per day: {n_articles[idx]/n_days:.2f}\")\n",
    "    print(\n",
    "        f\" - avg comments per article: {from_sample_avg_comments_per_articles[idx]:.2f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukraine coverage have been a continuous, long term effort, by *Le Monde,* with high interest from the public. Whereas I selected the **most active month** as a benchmark for *Retraites*, it has a very similar coverage/engagement to Ukraine where numbers are on a **one year** period. Re. *Trump,* which is not the least engaging topic, subscribers engagement level is more than twice as low than Ukraine.\n",
    "\n",
    "#### Misc : editorial share (type of articles)\n",
    "\n",
    "Excluding Lives. Didn't want to focus on editorial coverage, but here the share of articles types.\\\n",
    "I believe that \"factuel\" is the AFP news feed, but it needs confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our dataset has one row per comments\n",
    "# we're interested in share of article types only\n",
    "# Polars groupby + first() to keep article type value, then count()\n",
    "article_types = (\n",
    "    coms.groupby(by=\"article_id\").first().select([\"article_id\", \"article_type\"])\n",
    ")\n",
    "editorial_share = article_types.groupby(by=\"article_type\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| column: margin\n",
    "\n",
    "print(article_types)\n",
    "print(editorial_share.sort(\"count\", descending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-main\n",
    "#| fig-cap: 'Ukraine coverage, articles types (total: 2590 articles)'\n",
    "\n",
    "# Filter out least represented categories to lighten our pie chart viz\n",
    "editorial_share = editorial_share.filter(pl.col(\"count\") >= 40) \n",
    "\n",
    "labels = editorial_share.to_pandas()[\"article_type\"]\n",
    "values = editorial_share.to_pandas()[\"count\"]\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values)])\n",
    "fig.update_traces(\n",
    "    hoverinfo=\"label+percent+name\",\n",
    "    textinfo=\"value\",\n",
    "    textfont_size=12,\n",
    "    hole=0.3,\n",
    "    marker=dict(line=dict(color=\"#000000\", width=1)),\n",
    ")\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    #width=600\n",
    ")\n",
    "# title_text=\"Ukraine coverage (Febr 2022 - Febr 2023), \"articles types\", \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## People engagement on war over time, subscribers' comment activity as a proxy\n",
    "\n",
    "**TLDR,** people activity keeps being high independently of *Le Monde* articles frequency. Some peaks in activity would need further investigations but prob. tied to the usual remarkable events (offensives, nukes...). Before our analysis I would think that engagement would decrease, even slightly, over time but it seems not. Also, we're aware that comments as a proxy has biases[^4].\n",
    "\n",
    "[^4]: Le Monde 500k subscribers is a particular demographic, and only a handful of them are active authors.\n",
    "\n",
    "Code wise, our general workflow revolves around time series groupby / various aggregations using Polars, then convert back the results to Pandas for a quicker viz via Plotly. We will experiment with diverse metrics/windows to better render subscribers activity over this first year : *coms daily count, week / month avg, lines vs. hist, coms per article : daily / weekly, moving (rolling) mean on a 30 days period.*\n",
    "\n",
    "#### Daily number of comments, weekly, monthly averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. first thing first, number of comms per day\n",
    "coms_daily_count = coms.groupby(\"date\").count().sort(\"date\", descending=False)\n",
    "\n",
    "\n",
    "# 2. average number of comms per week (groupby window, using groupby_dynamic method in Polars)\n",
    "weekly_avg = coms_daily_count.groupby_dynamic(\"date\", every=\"1w\").agg(\n",
    "    [pl.col(\"count\").mean()]\n",
    ")\n",
    "\n",
    "# 3.same as above but per month\n",
    "monthly_avg = coms_daily_count.groupby_dynamic(\"date\", every=\"1mo\").agg(\n",
    "    [pl.col(\"count\").mean()]\n",
    ")\n",
    "# from left to right - average number of comments :\n",
    "# weekly avg (line), weekly avg (bars), monthly avg (line), monthly avg (bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| column: margin\n",
    "\n",
    "print(weekly_avg.head(3))\n",
    "print(monthly_avg.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| column: page\n",
    "#| layout-nrow: 1\n",
    "\n",
    "fig1 = px.line(\n",
    "    weekly_avg.to_pandas(),\n",
    "    x=\"date\",\n",
    "    y=\"count\",\n",
    "    #width=200,\n",
    "    height=300,\n",
    "    template=template,\n",
    ")\n",
    "fig2 = px.bar(\n",
    "    weekly_avg.to_pandas(),\n",
    "    x=\"date\",\n",
    "    y=\"count\",\n",
    "    #width=200,\n",
    "    height=300,\n",
    "    template=template,\n",
    ")\n",
    "fig3 = px.line(\n",
    "    monthly_avg.to_pandas(),\n",
    "    x=\"date\",\n",
    "    y=\"count\",\n",
    "    #width=600,\n",
    "    height=300,\n",
    "    template=template,\n",
    ")\n",
    "fig4 = px.bar(\n",
    "    monthly_avg.to_pandas(),\n",
    "    x=\"date\",\n",
    "    y=\"count\",\n",
    "    #width=600,\n",
    "    height=300,\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "fig3.show()\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower the impact of articles publication freq : ratio comm / articles, rolling mean\n",
    "\n",
    "When plotting the weekly / monthly avg of comments (above), we clearly distinguish 3 periods of high activity (start of conflict + 2 others), still with a sustained, constant readers involvement.\\\n",
    "But due to the number of comments prob. being tied to how many articles Le Monde published in the same time, lets visualize comments activity with normalization : coms per articles (removes articles frequency effect) and rolling mean (smoothen things out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily ratio comms per article. Still using Polars syntax >.<\n",
    "\n",
    "# 1. group by dates (daily), agg count articles, count comments\n",
    "daily_coms_per_articles = (\n",
    "    coms.groupby(by=\"date\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"article_id\").n_unique().alias(\"count_articles\"),\n",
    "            pl.col(\"comment\").count().alias(\"count_comments\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort(\"date\", descending=False)\n",
    ")\n",
    "\n",
    "# 2. then calculate coms per articles\n",
    "daily_coms_per_articles = daily_coms_per_articles.with_columns(\n",
    "    (pl.col(\"count_comments\") / pl.col(\"count_articles\")).alias(\"coms_per_article\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| column: margin\n",
    "print(daily_coms_per_articles.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly ratio coms per article. Polars method is .groupby_dynamic()\n",
    "\n",
    "weekly_coms_per_articles = (\n",
    "    coms.sort(\"date\", descending=False)\n",
    "    .groupby_dynamic(\"date\", every=\"1w\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"article_id\").n_unique().alias(\"count_articles\"),\n",
    "            pl.col(\"comment\").count().alias(\"count_comments\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort(\"date\", descending=False)\n",
    ")\n",
    "\n",
    "weekly_coms_per_articles = weekly_coms_per_articles.with_columns(\n",
    "    (pl.col(\"count_comments\") / pl.col(\"count_articles\")).alias(\"coms_per_article\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| column: margin\n",
    "print(weekly_coms_per_articles.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments activity keeps being high throughout the \"first\" year of conflict whatever the articles publication rhythm, with even a bigger number of comments per articles in the end period than in the very start.\\\n",
    "Some context : first two weeks of September : Ukraine counter-offensive in Karkhiv & Russian mobilization. January 2023 : battle tanks ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-main2\n",
    "#| fig-cap: Weekly comments per article\n",
    "px.bar(\n",
    "    weekly_coms_per_articles.to_pandas(),\n",
    "    x=\"date\",\n",
    "    y=\"coms_per_article\",\n",
    "    #width=600,\n",
    "    height=400,\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving (rolling) mean, another way to --kind of, smoothen out articles frequency, without the hassle above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_mean = coms_daily_count.with_columns(\n",
    "    pl.col(\"count\").rolling_mean(window_size=30).alias(\"moving_mean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| column: margin\n",
    "print(moving_mean.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-main3\n",
    "#| fig-cap: Rolling mean (windows_size=30) of daily count of comments\n",
    "px.bar(\n",
    "    moving_mean.to_pandas(),\n",
    "    x=\"date\",\n",
    "    y=\"moving_mean\",\n",
    "    #width=600,\n",
    "    height=400,\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who are the most active contributors ? Hardcore posters vs. the silent crowd\n",
    "\n",
    "::: {.callout-note appearance=\"simple\"}\n",
    "Fun fact: *goupil_hardi* acts as a true \"*sentinel*\" of the comments section, to a point where he *manually* counts & regularly cite the pro russian contributions under the articles. He is the one that made me decide to get the dataset and build this notebook.\n",
    ":::\n",
    "\n",
    "Could also do a lot of interesting stuff on trolls detection (if any, access to comments is pretty restricted) but we focused our efforts elsewhere.\n",
    "\n",
    "#### Top authors ; glad everyone is using a pseudonym ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top commentators\n",
    "authors = coms.groupby(\"author\").count().sort(\"count\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| column: margin\n",
    "print(authors.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Contribution shape, as expected, hardcore posters vs. the rest\n",
    "\n",
    "10 700 authors, average of 20 comments a year but the median is 4 coms only. Two authors with more than 2K comments. See how the top authors skew the distribution below.\n",
    "\n",
    "``` python\n",
    "authors.describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| column: margin\n",
    "print(authors.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-violin\n",
    "#| fig-cap: 'violin plot of authors, with all data points on the left side'\n",
    "fig_violin = px.violin(\n",
    "    authors.to_pandas(),\n",
    "    y=\"count\",\n",
    "    box=True,\n",
    "    points=\"all\", # add data points\n",
    "    #width=600,\n",
    "    height=400,\n",
    "    template=template,\n",
    ")\n",
    "fig_violin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| eval: false\n",
    "\n",
    "# histogram (filtered)\n",
    "histo = px.histogram(\n",
    "    authors.filter(pl.col(\"count\") <= 200).to_pandas(),\n",
    "    x=\"count\",\n",
    "    width=600,\n",
    "    height=400,\n",
    "    template=template,\n",
    ")\n",
    "# too many 'outliers' to visualize with a std box, keep first half of authors\n",
    "box = px.box(\n",
    "    authors.to_pandas()[5000:],\n",
    "    y=\"count\",\n",
    "    #width=800\n",
    "    height=400,\n",
    "    template=template\n",
    ")\n",
    "\n",
    "histo.show()\n",
    "box.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names that ring a bell\n",
    "\n",
    "If you had time to spare , you could do some semantic search / analysis on the arguments of each side. E.g the dissemination of pro-Russia arguments. But here a simple overview of selected authors & comments.\n",
    "\n",
    "*\"Goupil Hardi\",* second top poster with 2034 comments in 365 days (5 a days, on Ukraine only). Also not that the comment section is limited to one comment per author, per article + 3 replies-to-comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: asis\n",
    "# in Polars, unless I'm doing it wrong, it's harder than with Pandas to extract a col values.\n",
    "selected_coms = coms.select([\"author\", \"date\", \"comment\"]).filter(\n",
    "    pl.col(\"author\") == \"goupil hardi\").sample(n=3, seed=42).get_column(\"comment\").to_list()\n",
    "\n",
    "for i, com in enumerate(selected_coms):\n",
    "    print(f\"({i+1}) {com[0:90]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about *\"Lux\",* the top poster ? Don't remember seeing his name, but a similar profile --with less dedication ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "# looks like a strong supporter of Ukraine + wary about \"propagandists\".\n",
    "# Superficial analysis I know. Just curious.\n",
    "selected_coms2 = coms.select([\"author\", \"date\", \"comment\"]).filter(\n",
    "    pl.col(\"author\") == \"Lux\").sample(n=3, seed=10).get_column(\"comment\").to_list()\n",
    "\n",
    "for i, com in enumerate(selected_coms2):\n",
    "    print(f\"({i+1}) {com[0:90]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"Monod-Broca\".* Well, to each their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "# Denis Monod-Broca, also a name I remember of. Comments also on other topics to my recollection.\n",
    "# To the very least a strong defender of Russian invasion.\n",
    "selected_coms3 = coms.select([\"author\", \"date\", \"comment\"]).filter(\n",
    "    pl.col(\"author\") == \"Denis Monod-Broca\").sample(n=3, seed=58).get_column(\"comment\").to_list()\n",
    "\n",
    "for i, com in enumerate(selected_coms3):\n",
    "    print(f\"({i+1}) {com[0:90]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engagement through cohort analysis, what's about the retention rate ?\n",
    "\n",
    "A fancier way to analyze people engagement, over time, on the topic.\\\n",
    "Would be interesting to perform some benchmark on other topics.\n",
    "\n",
    "```         \n",
    "Steps\n",
    "1. add/get comment month -> month of the comment for each author\n",
    "2. add/get cohort month (first month that user posted a comment) \n",
    "    -> first month the authors commented = cohort creation\n",
    "3. add/get cohort index for each row\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone data to avoid recursive edit of our dataset\n",
    "cohort = coms.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder on how our original data looks like :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page\n",
    "#| echo: false\n",
    "print(cohort.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| column: margin\n",
    "# We will only use authors, date, number of comments to render our cohort\n",
    "# Also, switch to Pandas, more familiar with it for the following operations\n",
    "relevant_columns = [\"author\", \"date\", \"article_id\"]\n",
    "cohort = cohort.select(relevant_columns)\n",
    "cohort = cohort.to_pandas()\n",
    "cohort.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape to cohort (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| column: margin\n",
    "# 1. comment month\n",
    "# tip : map faster than apply, we can use it cause we're dealin with one col at a time\n",
    "cohort[\"comment_month\"] = cohort[\"date\"].map(lambda x: datetime(x.year, x.month, 1))\n",
    "display(cohort.head(2))\n",
    "\n",
    "# 2. cohort month\n",
    "# tip : transform after a groupby,return a df with the same length\n",
    "# and here return the min for each entry\n",
    "cohort[\"cohort_month\"] = cohort.groupby(\"author\")[\"comment_month\"].transform(\"min\")\n",
    "display(cohort.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| column: margin\n",
    "# 3. cohort index : for each row, difference in months,\n",
    "# between first comment month and cohort month\n",
    "def get_date(df, column):\n",
    "    year = df[column].dt.year\n",
    "    month = df[column].dt.month\n",
    "    day = df[column].dt.day\n",
    "    return year, month, day\n",
    "\n",
    "\n",
    "comment_year, comment_month, _ = get_date(cohort, \"comment_month\")\n",
    "cohort_year, cohort_month, _ = get_date(cohort, \"cohort_month\")\n",
    "year_diff = comment_year - cohort_year\n",
    "month_diff = comment_month - cohort_month\n",
    "cohort[\"cohort_index\"] = year_diff * 12 + month_diff + 1\n",
    "display(cohort.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohort active users (retention rate of authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final shaping groupby cohort_month * cohort_index, count (unique) authors\n",
    "# cohort active users (active authors / retention rate)\n",
    "active_authors = (\n",
    "    cohort.groupby([\"cohort_month\", \"cohort_index\"])[\"author\"]\n",
    "    .apply(pd.Series.nunique)\n",
    "    .reset_index()\n",
    ")\n",
    "active_authors = active_authors.pivot_table(\n",
    "    index=\"cohort_month\", columns=\"cohort_index\", values=\"author\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate cohort with Plotly, as a heatmap\n",
    "fig = px.imshow(\n",
    "    active_authors,\n",
    "    text_auto=True,\n",
    "    #width=1000,\n",
    "    height=500\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: column-page-inset-right\n",
    "::: panel-tabset\n",
    "## Active users (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| label: fig-cohort-count\n",
    "#| fig-cap: 'From left to right : our first cohort of authors (febr.) counts 2184 users. After 1 month 1677 are still active and after 12 months (first line last cell), 1050 are still commenting. Sept. cohort (row 8) : 513 new unique authors of which 63 only are active after 5 months'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "active_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    ":::\n",
    ":::\n",
    "\n",
    "#### Cohort percentage active users (retention rate, in %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# Clone previous dataframe (cohort users, count)\n",
    "active_authors_pct = active_authors.copy(deep=True)\n",
    "\n",
    "# get %\n",
    "active_authors_pct = active_authors.copy(deep=True)\n",
    "for col in active_authors_pct.columns[1:]:\n",
    "    active_authors_pct[col] = round(\n",
    "        active_authors_pct[col] / active_authors_pct[1] * 100, 2\n",
    "    )\n",
    "active_authors_pct[1] = 100\n",
    "\n",
    "# Generate heatmap (cohort users, %)\n",
    "labels = {\"x\": \"n months\", \"y\": \"cohort (by month)\", \"color\": \"% author\"}\n",
    "\n",
    "fig_pct = px.imshow(active_authors_pct, text_auto=True, labels=labels)\n",
    "fig_pct= fig_pct.update_xaxes(side=\"top\", ticks=\"outside\", tickson=\"boundaries\", ticklen=5)\n",
    "fig_pct = fig_pct.update_yaxes(showgrid=False)\n",
    "\n",
    "fig_pct = fig_pct.update_layout(\n",
    "    {\n",
    "        \"xaxis\": {\"tickmode\": \"linear\", \"showgrid\": False},\n",
    "        #\"width\": 800,\n",
    "        \"height\": 500,\n",
    "        \"plot_bgcolor\": \"rgba(0, 0, 0, 0)\",\n",
    "        \"paper_bgcolor\": \"rgba(0, 2, 0, 0)\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: column-page-inset-right\n",
    "::: panel-tabset\n",
    "## Active users (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| label: fig-cohort-pct\n",
    "#| fig-cap: Globally the retention rate is lower in later cohorts (authors that start posted later this year)\n",
    "fig_pct.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "active_authors_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    ":::\n",
    "\n",
    "## Comments embedding & fast retrieval using SBERT, Faiss\n",
    "\n",
    "Example of use : if we wanted to retrieve similar arguments / check propaganda --in an efficient way, with Faiss.\n",
    "\n",
    "#### Semantic search : resources & performance overview of our curated models\n",
    "\n",
    "-   *Notes and resources I found to be useful*\\\n",
    "    Model choice, must read : symmetric vs. asymmetric semantic search, language, tuning : [Sbert is all u need](https://medium.com/mlearning-ai/semantic-search-with-s-bert-is-all-you-need-951bc710e160)\\\n",
    "    Models available and metrics: [Sbert doc](https://www.sbert.net/docs/pretrained_models.html)\n",
    "\n",
    "-   *Misc*\\\n",
    "    Models trained for cosine prefer shorter document retrieval, vs. dot product (longer)\\\n",
    "    Faiss uses **inner product** (=dot product ; += cosine score if vectors are normalized e.g using faiss.normalize_L2 or **L2** to measure distances [(more here)](https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances#how-can-i-index-vectors-for-cosine-similarity).\\\n",
    "    In the first place, we were not sure of our typical use case : short query =\\> long comment (asymmetric), or comment =\\> similar comment (symmetric).\n",
    "\n",
    "-   *Candidate models we curated & tested :*\n",
    "\n",
    "    +---------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "    | Models                                | Quick notes                                                                                                                                                  |\n",
    "    +=======================================+==============================================================================================================================================================+\n",
    "    | paraphrase-multilingual-mpnet-base-v2 | multi languages, suitable score : optimized for cosine, max seq len 128                                                                                      |\n",
    "    +---------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "    | distiluse-base-multilingual-cased-v1  | symmetric, multi lang., max seq len 128, optimized for cosine                                                                                                |\n",
    "    +---------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "    | quora-distilbert-multilingual         | multilanguages, short text (questions) closest to our symm use case?\\                                                                                        |\n",
    "    |                                       | Example [here](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic-search/semantic_search_quora_pytorch.py) (pytorch) |\n",
    "    +---------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "    | dangvantuan/sentence-camembert-large  | Bigger model, symmetric ?, french, optimized l2 + tbc others ? Size embed 1024                                                                               |\n",
    "    +---------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "\n",
    "    : Curated models\n",
    "\n",
    "-   *Post run evaluation and remarks :*\\\n",
    "    `mpnet-base-v2`, `distiluse`, `quora` : fast encoding (20k documents \\< 1mn), results quite similar between models, each one finds our test query and pertinent results. A very good baseline.\\\n",
    "    `mpnet-base-v2`, `distiluse`, `quora` : with a flat inner product faiss index, no difference if we perform vectors normalization or not, maybe because they're optimized for cosine already?\\\n",
    "    `camembert` is a bigger model (1024 dimension), slower encoding (20k docs = 5mn), nice (better?) results (spoiler alert : it is optimized for French). With a flat IP index, if normalize = False, retrieve similar, *short* documents. If we normalize our embeddings, it retrieves our initial query + *longer*, similar documents.\n",
    "\n",
    "#### Fasten our models evaluation through prior rdm sampling, notes on speed.\n",
    "\n",
    "To speed our experiments up, we will work with a sample of comments (around 10% : 236k -\\> 20k)\\\n",
    "FYI,embedding of all comments (all 236k), takes approx 10mn on a 1080ti, i7700k, 32gb RAM, with curated models ; \\*1.5 to \\*2 when using the biggest model (`Camembert`).\\\n",
    "Encoding on our sample (10k) is \\< 1mn.\\\n",
    "No detailed measure on inference speed, but very fast with Faiss. Might want to try different --optimized, [indexes types](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index)with a bigger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, our dataset was loaded as a Polars dataframe,\n",
    "# sample method is very similar in Pandas though.\n",
    "coms_sample = coms.sample(seed=42, n=20000, shuffle=True)\n",
    "# We're removing articles content and some other cols we won't work with\n",
    "keep_cols = [\n",
    "    \"article_id\",\n",
    "    \"url\",\n",
    "    \"title\",\n",
    "    \"desc\",\n",
    "    \"date\",\n",
    "    \"keywords\",\n",
    "    \"author\",\n",
    "    \"comment\",\n",
    "]\n",
    "coms_sample = coms_sample.select(pl.col(keep_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: margin\n",
    "print(coms_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page\n",
    "#| echo: false\n",
    "print(coms_sample.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick cleaning, typically remove comments with 3 emojis only\n",
    "# Just filter out small comments\n",
    "coms_sample = coms_sample.filter(pl.col(\"comment\").str.n_chars() >= 45)\n",
    "\n",
    "# Finally, convert back to Pandas, for a \"better\" (re use of code;) workflow with Sbert and FAISS\n",
    "coms_sample = coms_sample.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: margin\n",
    "print(coms_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "#| echo: false\n",
    "#| output: false\n",
    "# Optional : on later stages as we will perform semantic search on comments\n",
    "# let's add a simple, order \"id\" column to our comments. Not needed eventually.\n",
    "comment_ids = list(range(0, coms_sample.shape[0]))\n",
    "coms_sample = coms_sample.with_columns(pl.Series(name=\"comment_id\", values=comment_ids))\n",
    "print(coms_sample.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convenience functions to repeat our experiments with different indexes / models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" comments embedding \"\"\"\n",
    "\n",
    "def comments_to_list(df, column: str) -> list[str]:\n",
    "    \"\"\"Extract documents from dataframe\"\"\"\n",
    "    return df[column].values.tolist()\n",
    "\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    \"\"\"Convenience fonction to load SBERT model\"\"\"\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "def encode_comments(model, comments):\n",
    "    \"\"\"Encode comments using previously loaded model\"\"\"\n",
    "    return model.encode(comments, show_progress_bar=True)\n",
    "\n",
    "\"\"\" create (a flat) Faiss index \"\"\"\n",
    "\n",
    "def create_faiss_index(embeddings, normalize: bool, index_type: str):\n",
    "    \"\"\"\n",
    "    Create a flat index in Faiss of index_type \"IP\" or \"L2\"\n",
    "    Index_types and prior vectors normalization varies\n",
    "    according model output optimization and task.\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    embeddings = np.array(embeddings).astype(\"float32\")\n",
    "    if normalize:\n",
    "        faiss.normalize_L2(embeddings)\n",
    "    if index_type == \"ip\":\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        index.add(embeddings)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "\n",
    "def save_index(index, filename: str):\n",
    "    \"\"\"Optional, save index to disk\"\"\"\n",
    "    faiss.write_index(index, f\"{filename}.index\")\n",
    "\n",
    "\n",
    "def load_index(filename):\n",
    "    \"\"\"Optional, load index from disk\"\"\"\n",
    "    return faiss.read_index(filename)\n",
    "\n",
    "\"\"\" query index \"\"\"\n",
    "\n",
    "def search_index(index, model, query:str, normalize: bool, top_k: int):\n",
    "    # encode query\n",
    "    vector = model.encode([query])\n",
    "    if normalize:\n",
    "        faiss.normalize_L2(vector)\n",
    "\n",
    "    # search with Faiss\n",
    "    Distances, Indexes = index.search(vector, top_k)\n",
    "    # Distances, Indexes = index.search(np.array(vector).astype(\"float32\"), top_k)\n",
    "    return Distances, Indexes\n",
    "\n",
    "\n",
    "def index_to_comments(df, column:str, Indexes):\n",
    "    \"\"\"Convenience function to retrieve top K comments\n",
    "    from our original Dataframe\n",
    "    \"\"\"\n",
    "    return df.iloc[Indexes[0]][column].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model, encode comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load comments (a list), pick our candidate model, load it\n",
    "comments = comments_to_list(coms_sample, \"comment\")\n",
    "model_name = \"paraphrase-multilingual-mpnet-base-v2\"\n",
    "model = load_model(model_name)\n",
    "normalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Encode comments. See notes above for elements of performance / speed\n",
    "embeddings = encode_comments(model, comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create (or load) our Faiss index, here a flat index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# create Faiss Index, here Flat Innner Product \n",
    "# (exhaustive search, \"no\" optimization)\n",
    "index = create_faiss_index(embeddings, normalize, index_type=\"ip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# optional : save Faiss index to disk\n",
    "filename = \"mpnet\"\n",
    "save_index(index, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, load from disk the previously saved Faiss index\n",
    "# so we do not rerun embeddings everytime we're executing the notebook\n",
    "# we found mpnet (multilang) to be a very good baseline for our dataset.\n",
    "filename = \"mpnet.index\"\n",
    "index = load_index(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's find similar comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract an existing comment (= will be our input query) from dataset\n",
    "print(coms_sample[\"comment\"].tolist()[1300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: asis\n",
    "# encode query, query index, retrieve top_k --here 8, nearest comments\n",
    "query = \"Quelle arrogance et quel cynisme. Qu'y a t-il de plus terroriste que la Russie d'aujourd'hui?\"\n",
    "top_k = 8\n",
    "Distances, Indexes = search_index(index, model, query, normalize, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display top similar comments\n",
    "results = index_to_comments(coms_sample, \"comment\", Indexes)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"{i+1}| : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Maybe later just for fun : 0 shot \"tone\" classification tests using OpenAI API"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,qmd:quarto"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
